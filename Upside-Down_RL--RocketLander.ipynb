{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upside-Down Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Traing on GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create episodes as namedtuple\n",
    "make_episode = namedtuple('Episode', \n",
    "                          field_names=['states', \n",
    "                                       'actions', \n",
    "                                       'rewards', \n",
    "                                       'init_command', \n",
    "                                       'total_return', \n",
    "                                       'length', \n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_replay_buffer(replay_size, n_episodes, last_few):\n",
    "    '''\n",
    "    Initialize replay buffer with warm-up episodes using random actions.\n",
    "    See section 2.3.1\n",
    "    \n",
    "    Params:\n",
    "        replay_size (int)\n",
    "        n_episodes (int)\n",
    "        last_few (int)\n",
    "    \n",
    "    Returns:\n",
    "        ReplayBuffer instance\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # This policy will generate random actions. Won't need state nor command\n",
    "    random_policy = lambda state, command: np.random.randint(env.action_space.n)\n",
    "    \n",
    "    buffer = ReplayBuffer(replay_size)\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(env, random_policy, command) # See Algorithm 2\n",
    "        buffer.add(episode)\n",
    "    \n",
    "    buffer.sort()\n",
    "    return buffer\n",
    "\n",
    "def initialize_behavior_function(state_size, \n",
    "                                 action_size, \n",
    "                                 hidden_size, \n",
    "                                 learning_rate, \n",
    "                                 command_scale):\n",
    "    '''\n",
    "    Initialize the behaviour function. See section 2.3.2\n",
    "    \n",
    "    Params:\n",
    "        state_size (int)\n",
    "        action_size (int)\n",
    "        hidden_size (int) -- NOTE: not used at the moment\n",
    "        learning_rate (float)\n",
    "        command_scale (List of float)\n",
    "    \n",
    "    Returns:\n",
    "        Behavior instance\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    behavior = Behavior(state_size, \n",
    "                        action_size, \n",
    "                        hidden_size, \n",
    "                        command_scale)\n",
    "    \n",
    "    behavior.init_optimizer(lr=learning_rate)\n",
    "    \n",
    "    return behavior\n",
    "\n",
    "def generate_episodes(env, behavior, buffer, n_episodes, last_few):\n",
    "    '''\n",
    "    1. Sample exploratory commands based on replay buffer\n",
    "    2. Generate episodes using Algorithm 2 and add to replay buffer\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        behavior (Behavior)\n",
    "        buffer (ReplayBuffer)\n",
    "        n_episodes (int)\n",
    "        last_few (int):\n",
    "            how many episodes we use to calculate the desired return and horizon\n",
    "    '''\n",
    "    \n",
    "    stochastic_policy = lambda state, command: behavior.action(state, command)\n",
    "    \n",
    "    for i in range(n_episodes_per_iter):\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(env, stochastic_policy, command) # See Algorithm 2\n",
    "        buffer.add(episode)\n",
    "    \n",
    "    # Let's keep this buffer sorted\n",
    "    buffer.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UDRL(env, buffer=None, behavior=None, learning_history=[]):\n",
    "    '''\n",
    "    Upside-Down Reinforcement Learning main algrithm\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        buffer (ReplayBuffer):\n",
    "            if not passed in, new buffer is created\n",
    "        behavior (Behavior):\n",
    "            if not passed in, new behavior is created\n",
    "        learning_history (List of dict) -- default []\n",
    "    '''\n",
    "    \n",
    "    if buffer is None:\n",
    "        buffer = initialize_replay_buffer(replay_size, \n",
    "                                          n_warm_up_episodes, \n",
    "                                          last_few)\n",
    "    \n",
    "    if behavior is None:\n",
    "        behavior = initialize_behavior_function(state_size, \n",
    "                                                action_size, \n",
    "                                                hidden_size, \n",
    "                                                learning_rate, \n",
    "                                                [return_scale, horizon_scale])\n",
    "    \n",
    "    for i in range(1, n_main_iter+1):\n",
    "        mean_loss = train_behavior(behavior, buffer, n_updates_per_iter, batch_size)\n",
    "        \n",
    "        print('Iter: {}, Loss: {:.4f}'.format(i, mean_loss), end='\\r')\n",
    "        \n",
    "        # Sample exploratory commands and generate episodes\n",
    "        generate_episodes(env, \n",
    "                          behavior, \n",
    "                          buffer, \n",
    "                          n_episodes_per_iter,\n",
    "                          last_few)\n",
    "        \n",
    "        if i % evaluate_every == 0:\n",
    "            command = sample_command(buffer, last_few)\n",
    "            mean_return = evaluate_agent(env, behavior, command)\n",
    "            \n",
    "            learning_history.append({\n",
    "                'training_loss': mean_loss,\n",
    "                'desired_return': command[0],\n",
    "                'desired_horizon': command[1],\n",
    "                'actual_return': mean_return,\n",
    "            })\n",
    "            \n",
    "            if stop_on_solved and mean_return >= target_return: \n",
    "                break\n",
    "    \n",
    "    return behavior, buffer, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    '''\n",
    "    Replay buffer containing a fixed maximun number of trajectories with \n",
    "    the highest returns seen so far\n",
    "    \n",
    "    Params:\n",
    "        size (int)\n",
    "    \n",
    "    Attrs:\n",
    "        size (int)\n",
    "        buffer (List of episodes)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size=0):\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        '''\n",
    "        Params:\n",
    "            episode (namedtuple):\n",
    "                (states, actions, rewards, init_command, total_return, length)\n",
    "        '''\n",
    "        \n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def get(self, num):\n",
    "        '''\n",
    "        Params:\n",
    "            num (int):\n",
    "                get the last `num` episodes from the buffer\n",
    "        '''\n",
    "        \n",
    "        return self.buffer[-num:]\n",
    "    \n",
    "    def random_batch(self, batch_size):\n",
    "        '''\n",
    "        Params:\n",
    "            batch_size (int)\n",
    "        \n",
    "        Returns:\n",
    "            Random batch of episodes from the buffer\n",
    "        '''\n",
    "        \n",
    "        idxs = np.random.randint(0, len(self), batch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    def sort(self):\n",
    "        '''Keep the buffer sorted in ascending order by total return'''\n",
    "        \n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.size:]\n",
    "    \n",
    "    def save(self, filename):\n",
    "        '''Save the buffer in numpy format\n",
    "        \n",
    "        Param:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        np.save(filename, self.buffer)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        '''Load a numpy format file\n",
    "        \n",
    "        Params:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        raw_buffer = np.load(filename)\n",
    "        self.size = len(raw_buffer)\n",
    "        self.buffer = \\\n",
    "            [make_episode(episode[0], episode[1], episode[2], episode[3], episode[4], episode[5]) \\\n",
    "             for episode in raw_buffer]\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            Size of the buffer\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    '''\n",
    "    Behavour function that produces actions based on a state and command.\n",
    "    NOTE: At the moment I'm fixing the amount of units and layers.\n",
    "    TODO: Make hidden layers configurable.\n",
    "    \n",
    "    Params:\n",
    "        state_size (int)\n",
    "        action_size (int)\n",
    "        hidden_size (int) -- NOTE: not used at the moment\n",
    "        command_scale (List of float)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale).to(device)\n",
    "        \n",
    "        self.state_fc = nn.Sequential(nn.Linear(state_size, 64), \n",
    "                                      nn.Tanh())\n",
    "        \n",
    "        self.command_fc = nn.Sequential(nn.Linear(2, 64), \n",
    "                                        nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = nn.Sequential(nn.Linear(64, 128), \n",
    "                                       nn.ReLU(), \n",
    "#                                        nn.Dropout(0.2),\n",
    "                                       nn.Linear(128, 128), \n",
    "                                       nn.ReLU(), \n",
    "#                                        nn.Dropout(0.2),\n",
    "                                       nn.Linear(128, 128), \n",
    "                                       nn.ReLU(), \n",
    "                                       nn.Linear(128, action_size))\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        '''Forward pass\n",
    "        \n",
    "        Params:\n",
    "            state (List of float)\n",
    "            command (List of float)\n",
    "        \n",
    "        Returns:\n",
    "            FloatTensor -- action logits\n",
    "        '''\n",
    "        \n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        '''\n",
    "        Params:\n",
    "            state (List of float)\n",
    "            command (List of float)\n",
    "            \n",
    "        Returns:\n",
    "            int -- stochastic action\n",
    "        '''\n",
    "        \n",
    "        logits = self.forward(state, command)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        return dist.sample().item()\n",
    "    \n",
    "    def greedy_action(self, state, command):\n",
    "        '''\n",
    "        Params:\n",
    "            state (List of float)\n",
    "            command (List of float)\n",
    "            \n",
    "        Returns:\n",
    "            int -- greedy action\n",
    "        '''\n",
    "        \n",
    "        logits = self.forward(state, command)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return np.argmax(probs.detach().cpu().numpy())\n",
    "    \n",
    "    def init_optimizer(self, optim=Adam, lr=0.003):\n",
    "        '''Initialize GD optimizer\n",
    "        \n",
    "        Params:\n",
    "            optim (Optimizer) -- default Adam\n",
    "            lr (float) -- default 0.003\n",
    "        '''\n",
    "        \n",
    "        self.optim = optim(self.parameters(), lr=lr)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        '''Save the model's parameters\n",
    "        Param:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        '''Load the model's parameters\n",
    "        \n",
    "        Params:\n",
    "            filename (str)\n",
    "        '''\n",
    "        \n",
    "        self.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, init_command=[1, 1]):\n",
    "    '''\n",
    "    Generate an episode using the Behaviour function.\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        policy (func)\n",
    "        init_command (List of float) -- default [1, 1]\n",
    "    \n",
    "    Returns:\n",
    "        Namedtuple (states, actions, rewards, init_command, total_return, length)\n",
    "    '''\n",
    "    \n",
    "    command = init_command.copy()\n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    time_steps = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    state = env.reset().tolist()\n",
    "    \n",
    "    while not done:\n",
    "        state_input = torch.FloatTensor(state).to(device)\n",
    "        command_input = torch.FloatTensor(command).to(device)\n",
    "        action = policy(state_input, command_input)\n",
    "        print(action)\n",
    "        returned_step = env.step(action)\n",
    "        print(returned_step)\n",
    "        next_state, reward, done, _ = returned_step\n",
    "        \n",
    "        # Modifying a bit the reward function punishing the agent, -100, \n",
    "        # if it reaches hyperparam max_steps. The reason I'm doing this \n",
    "        # is because I noticed that the agent tents to gather points by \n",
    "        # landing the spaceshipt and getting out and back in the landing \n",
    "        # area over and over again, never switching off the engines. \n",
    "        # The longer it does that the more reward it gathers. Later on in \n",
    "        # the training it realizes that it can get more points but turning \n",
    "        # off the engines, but takes more epochs to get to that conslusion.\n",
    "        if not done and time_steps > max_steps:\n",
    "            done = True\n",
    "            reward = max_steps_reward\n",
    "        \n",
    "        # Sparse rewards. Cumulative reward is delayed until the end of each episode\n",
    "#         total_rewards += reward\n",
    "#         reward = total_rewards if done else 0.0\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state.tolist()\n",
    "        \n",
    "        # Clipped such that it's upper-bounded by the maximum return achievable in the env\n",
    "        desired_return = min(desired_return - reward, max_reward)\n",
    "        \n",
    "        # Make sure it's always a valid horizon\n",
    "        desired_horizon = max(desired_horizon - 1, 1)\n",
    "    \n",
    "        command = [desired_return, desired_horizon]\n",
    "        time_steps += 1\n",
    "        \n",
    "    return make_episode(states, actions, rewards, init_command, sum(rewards), time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_behavior(behavior, buffer, n_updates, batch_size):\n",
    "    '''Training loop\n",
    "    \n",
    "    Params:\n",
    "        behavior (Behavior)\n",
    "        buffer (ReplayBuffer)\n",
    "        n_updates (int):\n",
    "            how many updates we're gonna perform\n",
    "        batch_size (int):\n",
    "            size of the bacth we're gonna use to train on\n",
    "    \n",
    "    Returns:\n",
    "        float -- mean loss after all the updates\n",
    "    '''\n",
    "    all_loss = []\n",
    "    for update in range(n_updates):\n",
    "        episodes = buffer.random_batch(batch_size)\n",
    "        \n",
    "        batch_states = []\n",
    "        batch_commands = []\n",
    "        batch_actions = []\n",
    "        \n",
    "        for episode in episodes:\n",
    "            T = episode.length\n",
    "            t1 = np.random.randint(0, T)\n",
    "            t2 = np.random.randint(t1+1, T+1)\n",
    "            dr = sum(episode.rewards[t1:t2])\n",
    "            dh = t2 - t1\n",
    "            \n",
    "            st1 = episode.states[t1]\n",
    "            at1 = episode.actions[t1]\n",
    "            \n",
    "            batch_states.append(st1)\n",
    "            batch_actions.append(at1)\n",
    "            batch_commands.append([dr, dh])\n",
    "        \n",
    "        batch_states = torch.FloatTensor(batch_states).to(device)\n",
    "        batch_commands = torch.FloatTensor(batch_commands).to(device)\n",
    "        batch_actions = torch.LongTensor(batch_actions).to(device)\n",
    "        \n",
    "        pred = behavior(batch_states, batch_commands)\n",
    "        \n",
    "        loss = F.cross_entropy(pred, batch_actions)\n",
    "        \n",
    "        behavior.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        behavior.optim.step()\n",
    "        \n",
    "        all_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_command(buffer, last_few):\n",
    "    '''Sample a exploratory command\n",
    "    \n",
    "    Params:\n",
    "        buffer (ReplayBuffer)\n",
    "        last_few:\n",
    "            how many episodes we're gonna look at to calculate \n",
    "            the desired return and horizon.\n",
    "    \n",
    "    Returns:\n",
    "        List of float -- command\n",
    "    '''\n",
    "    if len(buffer) == 0: return [1, 1]\n",
    "    \n",
    "    # 1.\n",
    "    commands = buffer.get(last_few)\n",
    "    \n",
    "    # 2.\n",
    "    lengths = [command.length for command in commands]\n",
    "    desired_horizon = round(np.mean(lengths))\n",
    "    \n",
    "    # 3.\n",
    "    returns = [command.total_return for command in commands]\n",
    "    mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "    desired_return = np.random.uniform(mean_return, mean_return+std_return)\n",
    "    \n",
    "    return [desired_return, desired_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, behavior, command, render=False):\n",
    "    '''\n",
    "    Evaluate the agent performance by running an episode\n",
    "    following Algorithm 2 steps\n",
    "    \n",
    "    Params:\n",
    "        env (OpenAI Gym Environment)\n",
    "        behavior (Behavior)\n",
    "        command (List of float)\n",
    "        render (bool) -- default False:\n",
    "            will render the environment to visualize the agent performance\n",
    "    '''\n",
    "    behavior.eval()\n",
    "    \n",
    "    print('\\nEvaluation.', end=' ')\n",
    "        \n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    print('Desired return: {:.2f}, Desired horizon: {:.2f}.'.format(desired_return, desired_horizon), end=' ')\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    for e in range(n_evals):\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = env.reset().tolist()\n",
    "    \n",
    "        while not done:\n",
    "            if render: env.render()\n",
    "            \n",
    "            state_input = torch.FloatTensor(state).to(device)\n",
    "            command_input = torch.FloatTensor(command).to(device)\n",
    "\n",
    "            action = behavior.greedy_action(state_input, command_input)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state.tolist()\n",
    "\n",
    "            desired_return = min(desired_return - reward, max_reward)\n",
    "            desired_horizon = max(desired_horizon - 1, 1)\n",
    "\n",
    "            command = [desired_return, desired_horizon]\n",
    "        \n",
    "        if render: env.close()\n",
    "        \n",
    "        all_rewards.append(total_reward)\n",
    "    \n",
    "    mean_return = np.mean(all_rewards)\n",
    "    print('Reward achieved: {:.2f}'.format(mean_return))\n",
    "    \n",
    "    behavior.train()\n",
    "    \n",
    "    return mean_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Rocket Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rocket_lander_gym\n",
    "\n",
    "env = gym.make('RocketLander-v0') # RocketLander-v0 | LunarLander-v2 | MountainCar-v0 | CartPole-v0\n",
    "# print(dir(env))\n",
    "# _ = env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 10\n",
      "Action size: 7\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State size: {}'.format(state_size))\n",
    "print('Action size: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations in the main loop\n",
    "n_main_iter = 5000\n",
    "\n",
    "# Number of (input, target) pairs per batch used for training the behavior function\n",
    "batch_size = 768\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "horizon_scale = 0.01\n",
    "\n",
    "# Number of episodes from the end of the replay buffer used for sampling exploratory\n",
    "# commands\n",
    "last_few = 75\n",
    "\n",
    "# Learning rate for the ADAM optimizer\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# Number of exploratory episodes generated per step of UDRL training\n",
    "n_episodes_per_iter = 20\n",
    "\n",
    "# Number of gradient-based updates of the behavior function per step of UDRL training\n",
    "n_updates_per_iter = 100\n",
    "\n",
    "# Number of warm up episodes at the beginning of training\n",
    "n_warm_up_episodes = 10\n",
    "\n",
    "# Maximum size of the replay buffer (in episodes)\n",
    "replay_size = 500\n",
    "\n",
    "# Scaling factor for desired return input\n",
    "return_scale = 20\n",
    "\n",
    "# Evaluate the agent after `evaluate_every` iterations\n",
    "evaluate_every = 10\n",
    "\n",
    "# Target return before breaking out of the training loop\n",
    "target_return = 1\n",
    "\n",
    "# Maximun reward given by the environment\n",
    "max_reward = 1\n",
    "\n",
    "# Maximun steps allowed\n",
    "max_steps = 300\n",
    "\n",
    "# Reward after reaching `max_steps` (punishment, hence negative reward)\n",
    "max_steps_reward = -50\n",
    "\n",
    "# Hidden units\n",
    "hidden_size = 32\n",
    "\n",
    "# Times we evaluate the agent\n",
    "n_evals = 1\n",
    "\n",
    "# Will stop the training when the agent gets `target_return` `n_evals` times\n",
    "stop_on_solved = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadrien/Documents/RL_lunar_lander/venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m behavior, buffer, learning_history \u001b[38;5;241m=\u001b[39m \u001b[43mUDRL\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mUDRL\u001b[0;34m(env, buffer, behavior, learning_history)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mUpside-Down Reinforcement Learning main algrithm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    learning_history (List of dict) -- default []\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_replay_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mn_warm_up_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mlast_few\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     behavior \u001b[38;5;241m=\u001b[39m initialize_behavior_function(state_size, \n\u001b[1;32m     21\u001b[0m                                             action_size, \n\u001b[1;32m     22\u001b[0m                                             hidden_size, \n\u001b[1;32m     23\u001b[0m                                             learning_rate, \n\u001b[1;32m     24\u001b[0m                                             [return_scale, horizon_scale])\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36minitialize_replay_buffer\u001b[0;34m(replay_size, n_episodes, last_few)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[1;32m     22\u001b[0m     command \u001b[38;5;241m=\u001b[39m sample_command(buffer, last_few)\n\u001b[0;32m---> 23\u001b[0m     episode \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# See Algorithm 2\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     buffer\u001b[38;5;241m.\u001b[39madd(episode)\n\u001b[1;32m     26\u001b[0m buffer\u001b[38;5;241m.\u001b[39msort()\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mgenerate_episode\u001b[0;34m(env, policy, init_command)\u001b[0m\n\u001b[1;32m     30\u001b[0m action \u001b[38;5;241m=\u001b[39m policy(state_input, command_input)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m---> 32\u001b[0m returned_step \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(returned_step)\n\u001b[1;32m     34\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m returned_step\n",
      "File \u001b[0;32m~/Documents/RL_lunar_lander/venv/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/RL_lunar_lander/venv/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RL_lunar_lander/venv/lib/python3.10/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Documents/RL_lunar_lander/venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    219\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdeprecation(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore environment is written in old step API which returns one bool instead of two. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is recommended to rewrite the environment with new step API. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[1;32m    223\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(done, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[1;32m    226\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `done` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(done)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/RL_lunar_lander/venv/lib/python3.10/site-packages/numpy/__init__.py:414\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "behavior, buffer, learning_history = UDRL(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ = UDRL(env, buffer, behavior, learning_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavior.save('behavior_rocket.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer.save('buffer_rocket.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('history_rocket.npy', learning_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_agent(env, behavior, [1, 240], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_with_monitor = gym.wrappers.Monitor(env, \"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(env_with_monitor, behavior, [1, 250], render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_return = [h['desired_return'] for h in learning_history]\n",
    "desired_horizon = [h['desired_horizon'] for h in learning_history]\n",
    "training_loss = [h['training_loss'] for h in learning_history]\n",
    "actual_return = [h['actual_return'] for h in learning_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axs[0, 0].plot(desired_return)\n",
    "axs[0, 0].set_title('Desired return')\n",
    "axs[0, 1].plot(desired_horizon)\n",
    "axs[0, 1].set_title('Desired horizon')\n",
    "axs[1, 0].plot(training_loss)\n",
    "axs[1, 0].set_title('Training loss')\n",
    "axs[1, 1].plot(actual_return)\n",
    "axs[1, 1].set_title('Actual return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16, 8))\n",
    "plt.plot(desired_return)\n",
    "plt.plot(actual_return)\n",
    "plt.legend(['Desired return', 'Actual return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HTML(\"\"\"\n",
    "<p style=\"text-align: center\">\n",
    "    <video width=\"600\" controls>\n",
    "        <source src=\"videos/LunarLander_with_UDRL_3.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "<p>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<p style=\"text-align: center\">\n",
    "    <video width=\"600\" controls>\n",
    "        <source src=\"videos/LunarLander_with_UDRL_4.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "<p>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
